{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node classification with Cora Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will need to deal with the node classification problem on Cora Dataset, which is similar to the Karate Club problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cora dataset consists of Machine Learning papers. These papers are classified into one of the following seven classes:\n",
    "- Case_Based\n",
    "- Genetic_Algorithms\n",
    "- Neural_Networks\n",
    "- Probabilistic_Methods\n",
    "- Reinforcement_Learning\n",
    "- Rule_Learning\n",
    "- Theory\n",
    "\n",
    "Each node represents a paper, and the link between them represents the citation relationship. It is splitted to train, validation and test set. This is also a common baseline for most graph neural network papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGLGraph(num_nodes=2708, num_edges=10556,\n",
      "         ndata_schemes={'train_mask': Scheme(shape=(), dtype=torch.float64), 'val_mask': Scheme(shape=(), dtype=torch.float64), 'test_mask': Scheme(shape=(), dtype=torch.float64), 'label': Scheme(shape=(), dtype=torch.int64), 'feat': Scheme(shape=(1433,), dtype=torch.float32)}\n",
      "         edata_schemes={})\n"
     ]
    }
   ],
   "source": [
    "from dgl.data import CoraDataset\n",
    "import torch as th\n",
    "dataset = CoraDataset()\n",
    "g = dataset[0]\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract masks and print dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node feature dimension: 1433\n",
      "Number of labels: 7\n",
      "Number of nodes for training: 140\n",
      "Number of nodes for validataion: 300\n",
      "Number of nodes for testing: 1000\n"
     ]
    }
   ],
   "source": [
    "num_labels = g.ndata['label'].max().item()+1 # label index started from 0\n",
    "feature_dim = g.ndata['feat'].shape[1]\n",
    "train_mask = g.ndata['train_mask'].to(th.bool)\n",
    "val_mask = g.ndata['val_mask'].to(th.bool)\n",
    "test_mask = g.ndata['test_mask'].to(th.bool)\n",
    "print(\"Node feature dimension: {}\".format(feature_dim))\n",
    "print(\"Number of labels: {}\".format(num_labels))\n",
    "print(\"Number of nodes for training: {}\".format(train_mask.long().sum()))\n",
    "print(\"Number of nodes for validataion: {}\".format(val_mask.long().sum()))\n",
    "print(\"Number of nodes for testing: {}\".format(test_mask.long().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Model and Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this challenge, you will need to modify the part below to achieve better performance on the test set. You can change the model structure, use other dgl [nn modules](https://docs.dgl.ai/api/python/nn.pytorch.html#module-dgl.nn.pytorch.conv), tuning hyperparameters in optimizers, add early stopping and so on.  \n",
    "**However, please remember only using training data in the training loop below**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 2-layer GCN model with DGL nn modules\n",
    "from dgl.nn.pytorch import conv as dgl_conv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        # GraphConv did more than GCNLayer defined above, it also added normalization for each node.\n",
    "        # Details can be found in original paper https://arxiv.org/abs/1609.02907\n",
    "        self.gcn1 = dgl_conv.GraphConv(in_feats, hidden_size, activation=F.relu) \n",
    "        self.gcn2 = dgl_conv.GraphConv(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        h = self.gcn1(g, inputs)\n",
    "        h = self.gcn2(g, h)\n",
    "        return h\n",
    "    \n",
    "net = GCN(1433, 64, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0    |  Loss: 1.9460  |  Train acc: 0.1714  |  Val acc: 0.1167\n",
      "Epoch 1    |  Loss: 1.9197  |  Train acc: 0.4071  |  Val acc: 0.3700\n",
      "Epoch 2    |  Loss: 1.8883  |  Train acc: 0.4286  |  Val acc: 0.3800\n",
      "Epoch 3    |  Loss: 1.8510  |  Train acc: 0.4357  |  Val acc: 0.3967\n",
      "Epoch 4    |  Loss: 1.8118  |  Train acc: 0.4357  |  Val acc: 0.4033\n",
      "Epoch 5    |  Loss: 1.7718  |  Train acc: 0.4357  |  Val acc: 0.4033\n",
      "Epoch 6    |  Loss: 1.7322  |  Train acc: 0.4357  |  Val acc: 0.4033\n",
      "Epoch 7    |  Loss: 1.6940  |  Train acc: 0.4357  |  Val acc: 0.4033\n",
      "Epoch 8    |  Loss: 1.6575  |  Train acc: 0.4357  |  Val acc: 0.4067\n",
      "Epoch 9    |  Loss: 1.6220  |  Train acc: 0.4500  |  Val acc: 0.4267\n",
      "Epoch 10   |  Loss: 1.5864  |  Train acc: 0.4571  |  Val acc: 0.4300\n",
      "Epoch 11   |  Loss: 1.5494  |  Train acc: 0.4714  |  Val acc: 0.4500\n",
      "Epoch 12   |  Loss: 1.5102  |  Train acc: 0.5000  |  Val acc: 0.4667\n",
      "Epoch 13   |  Loss: 1.4682  |  Train acc: 0.5071  |  Val acc: 0.4667\n",
      "Epoch 14   |  Loss: 1.4234  |  Train acc: 0.5500  |  Val acc: 0.4700\n",
      "Epoch 15   |  Loss: 1.3765  |  Train acc: 0.5714  |  Val acc: 0.4933\n",
      "Epoch 16   |  Loss: 1.3281  |  Train acc: 0.6143  |  Val acc: 0.5067\n",
      "Epoch 17   |  Loss: 1.2787  |  Train acc: 0.6714  |  Val acc: 0.5400\n",
      "Epoch 18   |  Loss: 1.2290  |  Train acc: 0.7286  |  Val acc: 0.5833\n",
      "Epoch 19   |  Loss: 1.1794  |  Train acc: 0.7571  |  Val acc: 0.6133\n",
      "Epoch 20   |  Loss: 1.1296  |  Train acc: 0.8071  |  Val acc: 0.6300\n",
      "Epoch 21   |  Loss: 1.0799  |  Train acc: 0.8357  |  Val acc: 0.6400\n",
      "Epoch 22   |  Loss: 1.0301  |  Train acc: 0.8500  |  Val acc: 0.6667\n",
      "Epoch 23   |  Loss: 0.9805  |  Train acc: 0.8643  |  Val acc: 0.6733\n",
      "Epoch 24   |  Loss: 0.9312  |  Train acc: 0.8786  |  Val acc: 0.6967\n",
      "Epoch 25   |  Loss: 0.8825  |  Train acc: 0.8786  |  Val acc: 0.7133\n",
      "Epoch 26   |  Loss: 0.8348  |  Train acc: 0.8857  |  Val acc: 0.7300\n",
      "Epoch 27   |  Loss: 0.7881  |  Train acc: 0.8857  |  Val acc: 0.7367\n",
      "Epoch 28   |  Loss: 0.7427  |  Train acc: 0.9071  |  Val acc: 0.7467\n",
      "Epoch 29   |  Loss: 0.6986  |  Train acc: 0.9071  |  Val acc: 0.7567\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=0.01)\n",
    "loss_fcn = th.nn.CrossEntropyLoss()\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    logits = net(g, g.ndata['feat'])\n",
    "    loss = loss_fcn(logits[train_mask], g.ndata['label'][train_mask])\n",
    "    train_acc = (logits.argmax(1)==g.ndata['label'])[train_mask].float().mean()\n",
    "    val_acc = (logits.argmax(1)==g.ndata['label'])[val_mask].float().mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:<4} |  Loss: {:.4f}  |  Train acc: {:.4f}  |  Val acc: {:.4f}'.format(epoch, loss.item(), train_acc, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate result on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.6880\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "logits = net(g, g.ndata['feat'])\n",
    "test_acc = (logits.argmax(1)==g.ndata['label'])[test_mask].float().mean()\n",
    "print('Test accuracy: {:.4f}'.format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
