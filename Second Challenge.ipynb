{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Challenge\n",
    "\n",
    "The second challenge would be learning the node embeddings of CORA dataset in an **unsupervised** way, i.e. the node labels would not be available during training time.\n",
    "\n",
    "We measure the performance of the embedding matrix by training a simple softmax classifier on the learned item embeddings on the training labels, and compute the accuracy on the test labels.  However, remember that both the training, validation, and test labels are **unavailable** during training; you MUST NOT use them.  Instead, please treat the evaluation routine as a black box, and only run the routine at test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE THIS CELL\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import scipy.sparse as ssp\n",
    "import dgl\n",
    "import dgl.data\n",
    "import dgl.nn.pytorch as dglnn\n",
    "from collections import namedtuple\n",
    "\n",
    "Args = namedtuple('Args', ['dataset'])\n",
    "dataset = dgl.data.load_data(Args('cora'))\n",
    "\n",
    "G = dgl.DGLGraph(dataset.graph)\n",
    "X = torch.FloatTensor(dataset.features)\n",
    "\n",
    "def evaluate(emb):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of the learned embedding.  The greater returned\n",
    "    value the better.\n",
    "    \n",
    "    It trains a softmax regression model on the training set from the given\n",
    "    embeddings, and return the accuracy on the test set.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    emb : numpy.ndarray\n",
    "        An N-by-M matrix where N is the number of nodes in CORA and M is\n",
    "        the size of node embedding (can be of any value).\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LogisticRegressionCV\n",
    "    global dataset\n",
    "    C = LogisticRegressionCV(\n",
    "        Cs=[1e-3, 1e-2, 1e-1, 1, 10, 100, 1000, 10000, 100000, 1e+6, 1e+7],\n",
    "        multi_class='multinomial', solver='lbfgs', max_iter=10000)\n",
    "    train_mask = (dataset.train_mask != 0)\n",
    "    test_mask = (dataset.test_mask != 0)\n",
    "    labels = dataset.labels\n",
    "    C.fit(emb[train_mask], labels[train_mask])\n",
    "    print('Best model found with C =', C.C_[0])\n",
    "    return C.score(emb[test_mask], labels[test_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect you to learn the node embeddings only from the given graph `G` and the node features `X`.  The following cell is an example solution which does nothing.  Please implement your model and report the number when you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gq/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model found with C = 100000.0\n",
      "Baseline performance using raw features: 0.578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gq/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model found with C = 100000.0\n",
      "Baseline performance using my embedding: 0.578\n"
     ]
    }
   ],
   "source": [
    "embedding = X.numpy()\n",
    "print('Baseline performance using raw features:', evaluate(X.numpy()))\n",
    "print('Baseline performance using my embedding:', evaluate(embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
